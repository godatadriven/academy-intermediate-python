{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3564b22d",
   "metadata": {},
   "source": [
    "# Working with Files in Python\n",
    "\n",
    "This notebook will teach you how to:\n",
    "- [Navigate directories using `pathlib` and `os`](#pathlib)\n",
    "- [Read different file formats](#format)\n",
    "- [Work with multiple datasets from a folder](#files)\n",
    "- [Exercises](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04964c",
   "metadata": {},
   "source": [
    "<a id=pathlib></a>\n",
    "\n",
    "## Navigating Directories\n",
    "\n",
    "### Using `os` module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc796444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in a directory\n",
    "files = os.listdir('data')\n",
    "print(f\"\\nFiles in data/: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6661185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file exists\n",
    "file_exists = os.path.exists('data/sales.csv')\n",
    "print(f\"\\nDoes sales.csv exist? {file_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52054ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file size\n",
    "if file_exists:\n",
    "    file_size = os.path.getsize('data/sales.csv')\n",
    "    print(f\"File size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ce5df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Using `pathlib` (Modern Python approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directory\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path object\n",
    "data_dir = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files\n",
    "files = list(data_dir.iterdir())\n",
    "print(f\"\\nFiles in data/:\")\n",
    "for file in files:\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "sales_file = data_dir / 'sales.csv'\n",
    "print(f\"\\nDoes sales.csv exist? {sales_file.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file info\n",
    "if sales_file.exists():\n",
    "    print(f\"File size: {sales_file.stat().st_size} bytes\")\n",
    "    print(f\"Is it a file? {sales_file.is_file()}\")\n",
    "    print(f\"Is it a directory? {sales_file.is_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67abab",
   "metadata": {},
   "source": [
    "<a id=format></a>\n",
    "\n",
    "## Reading Different File Types\n",
    "\n",
    "### Reading CSV Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "df_csv = pd.read_csv('data/sales.csv')\n",
    "\n",
    "print(\"CSV file contents:\")\n",
    "print(df_csv.head())\n",
    "print(f\"\\nShape: {df_csv.shape}\")\n",
    "print(f\"Columns: {df_csv.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67747bc6",
   "metadata": {},
   "source": [
    "### Reading JSON Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa720d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"JSON file contents:\")\n",
    "print(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac79b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access nested values\n",
    "print(f\"\\nDatabase host: {config['database']['host']}\")\n",
    "print(f\"API endpoint: {config['api']['endpoint']}\")\n",
    "print(f\"Regions: {config['regions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a8b33",
   "metadata": {},
   "source": [
    "\n",
    "### Reading Parquet Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1df07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read Parquet file\n",
    "df_parquet = pd.read_parquet('data/large_sales.parquet')\n",
    "\n",
    "print(\"Parquet file contents:\")\n",
    "print(df_parquet.head())\n",
    "print(f\"\\nShape: {df_parquet.shape}\")\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(df_parquet.memory_usage(deep=True))\n",
    "\n",
    "# Parquet files are more efficient for large datasets\n",
    "print(f\"\\nFirst transaction: {df_parquet.iloc[0]['date']}\")\n",
    "print(f\"Last transaction: {df_parquet.iloc[-1]['date']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03265b",
   "metadata": {},
   "source": [
    "### Reading Text Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Read entire file as string\n",
    "with open('data/system_log.txt', 'r') as f:\n",
    "    log_content = f.read()\n",
    "\n",
    "print(\"Text file contents:\")\n",
    "print(log_content[:200])  # First 200 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19649ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Read line by line\n",
    "with open('data/system_log.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(\"\\nFirst 3 lines:\")\n",
    "for line in lines[:3]:\n",
    "    print(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Process line by line (memory efficient for large files)\n",
    "error_lines = []\n",
    "with open('data/system_log.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if 'ERROR' in line:\n",
    "            error_lines.append(line.strip())\n",
    "\n",
    "print(f\"\\nFound {len(error_lines)} error lines:\")\n",
    "for error in error_lines:\n",
    "    print(f\"  {error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f9286",
   "metadata": {},
   "source": [
    "### Reading YAML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Read YAML file\n",
    "with open('data/pipeline_config.yml', 'r') as f:\n",
    "    pipeline_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"YAML file contents:\")\n",
    "print(f\"Pipeline name: {pipeline_config['pipeline_name']}\")\n",
    "print(f\"Version: {pipeline_config['version']}\")\n",
    "print(f\"Schedule: {pipeline_config['schedule']}\")\n",
    "\n",
    "print(\"\\nPipeline stages:\")\n",
    "for stage in pipeline_config['stages']:\n",
    "    print(f\"  - {stage['name']}: {stage['enabled']}\")\n",
    "\n",
    "print(f\"\\nRetry policy: {pipeline_config['retry_policy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960cf55",
   "metadata": {},
   "source": [
    "<a id=files></a>\n",
    "\n",
    "## Multiple CSV Files from a Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using `pathlib` and `pandas.concat()`\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Get all CSV files in the monthly_sales folder\n",
    "sales_folder = Path('data/monthly_sales')\n",
    "csv_files = list(sales_folder.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# Read all files and combine them\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # Add a column to track which file the data came from\n",
    "    df['source_file'] = file.stem  # stem gives filename without extension\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {combined_df.shape}\")\n",
    "print(f\"Total rows: {len(combined_df)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(combined_df.head())\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(combined_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645a2ee",
   "metadata": {},
   "source": [
    "## <mark>Exercises</mark>\n",
    "\n",
    "### <mark>Exercise 1: Filter and Combine Log Files</mark>\n",
    "\n",
    "You have multiple log files from different servers in the `data/logs/` folder. Your task is to:\n",
    "1. Read all text files from the folder\n",
    "2. Extract only the ERROR and WARNING lines\n",
    "3. Create a DataFrame with columns: `timestamp`, `level`, `message`\n",
    "4. Sort by timestamp\n",
    "\n",
    "Your expected output is as follows:\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "   timestamp            level  message                          server\n",
    "0  2024-01-15 10:24:12  ERROR  Database connection failed      server1\n",
    "1  2024-01-15 10:25:03  WARNING High memory usage: 85%         server1\n",
    "2  2024-01-15 10:27:45  ERROR  Timeout on API call             server1\n",
    "3  2024-01-15 10:31:22  WARNING Disk space low: 15%            server2\n",
    "4  2024-01-15 10:32:33  ERROR  Failed to write file            server2\n",
    "5  2024-01-15 10:41:11  ERROR  Network timeout                 server3\n",
    "6  2024-01-15 10:42:22  WARNING CPU usage: 95%                 server3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Your code here\n",
    "# 1. Get all .log files from data/logs/\n",
    "# 2. Read each file and extract ERROR and WARNING lines\n",
    "# 3. Parse each line into timestamp, level, and message\n",
    "# 4. Create a DataFrame and sort by timestamp\n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f676a",
   "metadata": {},
   "source": [
    "### <mark>Exercise 2: Configuration Merger</mark>\n",
    "\n",
    "You have multiple JSON configuration files for different environments (dev, staging, prod). Your task is to:\n",
    "1. Read all JSON files from `data/configs/`\n",
    "2. Merge them into a single DataFrame showing settings across environments\n",
    "3. Identify which settings differ between environments\n",
    "\n",
    "Your expected output is as follows:\n",
    "```\n",
    "                        dev                      staging                  prod\n",
    "database_host           localhost                staging-db.company.com   prod-db.company.com\n",
    "database_port           5432                     5432                     5432\n",
    "database_max_connections 10                      50                       200\n",
    "api_timeout             30                       60                       60\n",
    "api_rate_limit          100                      500                      1000\n",
    "debug                   True                     True                     False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# TODO: Your code here\n",
    "# 1. Read all JSON files from data/configs/\n",
    "# 2. Flatten the nested structure\n",
    "# 3. Create a DataFrame where each row is a setting and columns are environments\n",
    "# 4. Identify settings that differ across environments\n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadd7f6",
   "metadata": {},
   "source": [
    "**Answers**: Uncomment and run the code to see answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/file-1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/file-2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc462e67",
   "metadata": {},
   "source": [
    "## Summary: File Reading Best Practices\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Use `pathlib`** over `os` for modern, cleaner code\n",
    "- **CSV files**: Use `pd.read_csv()` for tabular data\n",
    "- **JSON files**: Use `json.load()` for configs, `pd.read_json()` for tabular data\n",
    "- **Parquet files**: Use `pd.read_parquet()` for efficient storage of large datasets\n",
    "- **Text files**: Use context managers (`with open()`) to ensure files close properly\n",
    "- **YAML files**: Use `yaml.safe_load()` for configuration files\n",
    "- **Multiple files**: Use `pathlib.glob()` + `pd.concat()` to combine datasets\n",
    "\n",
    "**Common patterns:**\n",
    "```python\n",
    "# Read single CSV\n",
    "df = pd.read_csv('file.csv')\n",
    "\n",
    "# Read all CSVs from folder\n",
    "dfs = [pd.read_csv(f) for f in Path('folder').glob('*.csv')]\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Read config file\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "```\n",
    "\n",
    "**Remember**: Always use context managers (`with open()`) when reading files to ensure they're properly closed, even if an error occurs! üìÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
