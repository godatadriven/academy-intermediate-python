{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3564b22d",
   "metadata": {},
   "source": [
    "# Working with Files in Python\n",
    "\n",
    "When working with Python, it is possible to access external files. This notebook covers how to access different types of files in the best way as well as how to navigate and interact with your directories from Python.\n",
    "\n",
    "Contents:\n",
    "- [Navigate directories using `os` and `pathlib`](#pathlib)\n",
    "- [Read different file formats](#format)\n",
    "- [Work with multiple datasets from a folder](#files)\n",
    "- [Exercises](#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04964c",
   "metadata": {},
   "source": [
    "<a id=pathlib></a>\n",
    "\n",
    "## Navigating Directories\n",
    "\n",
    "### Using `os` module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc796444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in a directory\n",
    "files = os.listdir('data')\n",
    "print(f\"\\nFiles in data/: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6661185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a file exists\n",
    "file_exists = os.path.exists('data/sales.csv')\n",
    "print(f\"\\nDoes sales.csv exist? {file_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52054ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file size\n",
    "if file_exists:\n",
    "    file_size = os.path.getsize('data/sales.csv')\n",
    "    print(f\"File size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ce5df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Using `pathlib` (Modern Python approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directory\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path object\n",
    "data_dir = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files\n",
    "files = list(data_dir.iterdir())\n",
    "print(f\"\\nFiles in data/:\")\n",
    "for file in files:\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "sales_file = data_dir / 'sales.csv'\n",
    "print(f\"\\nDoes sales.csv exist? {sales_file.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file info\n",
    "if sales_file.exists():\n",
    "    print(f\"File size: {sales_file.stat().st_size} bytes\")\n",
    "    print(f\"Is it a file? {sales_file.is_file()}\")\n",
    "    print(f\"Is it a directory? {sales_file.is_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67abab",
   "metadata": {},
   "source": [
    "<a id=format></a>\n",
    "\n",
    "## Reading Different File Types\n",
    "\n",
    "There are many different file types you will need to work with when working in Python. The obvious and sometimes first is a `.csv` or `.xlsx` file with pandas. However there are other types that will be used for different purposes.\n",
    "\n",
    "- [Comma-separated values](#csv) `CSV`: A tabular data format where each column is separated by a comma\n",
    "    - Commonly used with internal data\n",
    "\n",
    "- Text `txt`: Plain text files ‚Äì a simple format containing unstructured or semi-structured data\n",
    "    - Often used for logs, notes, or simple datasets\n",
    "\n",
    "- JSON `.json`: JavaScript Object Notation ‚Äì a structured, hierarchical format using key-value pairs\n",
    "    - Commonly used for APIs, configuration files, and modern data interchange\n",
    "\n",
    "- YAML `.yaml` / `.yml`: ‚ÄúYAML Ain‚Äôt Markup Language‚Äù ‚Äì a human-readable structured format similar to JSON but easier to read\n",
    "    - Often used for configuration files, pipeline definitions, and infrastructure-as-code\n",
    "\n",
    "- Parquet `.parquet`: Columnar, binary data format optimized for analytics and big data processing\n",
    "    - Commonly used in data engineering workflows for large datasets because it‚Äôs fast and memory-efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41654eb6",
   "metadata": {},
   "source": [
    "\n",
    "<a id=csv></a>\n",
    "\n",
    "### Reading CSV Files\n",
    "\n",
    "Pandas provides a built-in function to read CSVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "sales = pd.read_csv('data/sales.csv', header=2)\n",
    "\n",
    "print(\"CSV file contents:\")\n",
    "print(sales.head())\n",
    "print(f\"\\nShape: {sales.shape}\")\n",
    "print(f\"Columns: {', '.join(sales.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae983b",
   "metadata": {},
   "source": [
    "And can even help when the file formatting isn't quite a straight-forward as one would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8923cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'data/employee_data.csv',\n",
    "    skiprows=2,                 # skip the comment lines\n",
    "    sep='|',                     # delimiter is a pipe\n",
    "    usecols=['ID', 'Name', 'Salary', 'Start Date'], \n",
    "    dtype={'ID': str},           # ensure ID stays a string\n",
    "    parse_dates=['Start Date'],   # parse Start Date as datetime\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f941eb",
   "metadata": {},
   "source": [
    "### Reading Text Files\n",
    "\n",
    "Knowing the best practices for reading text files in Python is important because it ensures your code is safe, efficient, and portable, preventing common issues like memory overload, encoding errors, or leaving files open accidentally.\n",
    "\n",
    "Best practices include:\n",
    "- Using the `with` context manager\n",
    "- Use f.read() on the entire content for small files and line-by-line for larger files\n",
    "- Handle exceptions (more on that in the next notebook)\n",
    "- Avoid hard-coding paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a6220",
   "metadata": {},
   "source": [
    "Let's say you have a log file saved and you want to check how many errors have occurred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eae529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Read entire file as string\n",
    "with open('data/system_log.txt', 'r') as f:\n",
    "    log_content = f.read()\n",
    "\n",
    "print(\"Text file contents:\")\n",
    "print(log_content[:200])  # First 200 characters\n",
    "print('\\nNumber of errors:', log_content.count('ERROR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12523336",
   "metadata": {},
   "source": [
    "Another option is to read the data line-by-line to provide quick analysis like counting lines, checking content, or searching for specific keywords. Reading all lines into a list allows multiple passes over the data without reopening the file. This can be helpful when exploring a new dataset or log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579817da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Read line by line\n",
    "with open('data/system_log.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "num_errors = len([lin for lin in lines if 'ERROR' in lin])\n",
    "\n",
    "print(\"\\nFirst 3 lines:\")\n",
    "print(''.join(lines[:3]))\n",
    "\n",
    "print('\\nNumber of errors:', num_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988cea8",
   "metadata": {},
   "source": [
    "If `system_log.txt` is very large (hundreds of MBs or GBs), `f.readlines()` can use a lot of memory. In that case, it‚Äôs better to iterate over the file line by line instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86836b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Process line by line (memory efficient for large files)\n",
    "error_lines = []\n",
    "with open('data/system_log.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if 'ERROR' in line:\n",
    "            error_lines.append(line.strip())\n",
    "\n",
    "print(f\"\\nFound {len(error_lines)} error lines:\")\n",
    "for error in error_lines:\n",
    "    print(f\"  {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67747bc6",
   "metadata": {},
   "source": [
    "### Reading JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) is widely used in Python because it‚Äôs a lightweight, human-readable way to represent structured data. The most common use cases are configuration files, API requests or storing structured data.\n",
    "\n",
    "Here is a configuration example, where the `json` library is used to convert the JSON file into a Python dictionary, so you can access nested values using standard dictionary syntax instead of parsing text manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa720d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"JSON file contents:\")\n",
    "print(json.dumps(config, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98663e9",
   "metadata": {},
   "source": [
    "Now if you want to access the nested values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac79b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDatabase host: {config['database']['host']}\")\n",
    "print(f\"API endpoint: {config['api']['endpoint']}\")\n",
    "print(f\"Regions: {config['regions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a8b33",
   "metadata": {},
   "source": [
    "### Reading Parquet Files\n",
    "\n",
    "Reading a Parquet file like this is useful because Parquet is a columnar, compressed, binary format designed for large datasets:\n",
    "\n",
    "- Faster reads/writes than CSV or JSON because only the needed columns are loaded.\n",
    "- Smaller disk space usage due to compression.\n",
    "- Preserves data types (e.g., dates, integers) better than CSV.\n",
    "\n",
    "Parquet + Pandas works fine for medium datasets, but Polars shines for very large datasets in both speed and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1df07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Read Parquet file\n",
    "df_parquet = pl.read_parquet('data/large_sales.parquet')\n",
    "\n",
    "print(\"Parquet file contents:\")\n",
    "print(df_parquet.head())  # Polars DataFrame head\n",
    "\n",
    "# Shape\n",
    "print(f\"\\nShape: {df_parquet.shape}\")\n",
    "\n",
    "# Memory usage (approximate, Polars doesn't have exact equivalent of Pandas deep=True)\n",
    "print(f\"\\nEstimated memory usage: {df_parquet.estimated_size() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Access first and last transaction dates\n",
    "print(f\"\\nFirst transaction: {df_parquet[0, 'date']}\")\n",
    "print(f\"Last transaction: {df_parquet[-1, 'date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f9286",
   "metadata": {},
   "source": [
    "### Reading YAML Files\n",
    "\n",
    "YAML files are used for storing structured, human-readable configuration data, often for applications, pipelines, or infrastructure. They are like JSON but easier for humans to read and write, supporting comments, nested structures, and lists more cleanly.\n",
    "\n",
    "Using Python you can:\n",
    "- Easily parse structured data:\n",
    "- Convert YAML into a Python dictionary and nested lists, so you can access values naturally using dict syntax.\n",
    "- Keep configuration separate from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Read YAML file\n",
    "with open('data/pipeline_config.yml', 'r') as f:\n",
    "    pipeline_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"YAML file contents:\")\n",
    "print(f\"Pipeline name: {pipeline_config['pipeline_name']}\")\n",
    "print(f\"Version: {pipeline_config['version']}\")\n",
    "print(f\"Schedule: {pipeline_config['schedule']}\")\n",
    "\n",
    "print(\"\\nPipeline stages:\")\n",
    "for stage in pipeline_config['stages']:\n",
    "    print(f\"  - {stage['name']}: {stage['enabled']}\")\n",
    "\n",
    "print(f\"\\nRetry policy: {pipeline_config['retry_policy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960cf55",
   "metadata": {},
   "source": [
    "<a id=files></a>\n",
    "\n",
    "## Multiple CSV Files from a Folder\n",
    "\n",
    "Combining these things together, using `pathlib` and reading file types, you can start to apply more logic. For example, let's say you receive monthly company sales files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1aff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Get all CSV files in the monthly_sales folder\n",
    "sales_folder = Path('data/monthly_sales')\n",
    "csv_files = list(sales_folder.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65f445",
   "metadata": {},
   "source": [
    "\n",
    "Now let's say you want to read this all in as one collection of data to do full analysis on it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files and combine them\n",
    "sales_dfs = [\n",
    "    pd.read_csv(file).assign(source_file = file.stem)\n",
    "    for file in csv_files\n",
    "]\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_sales = pd.concat(sales_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {combined_sales.shape}\")\n",
    "combined_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645a2ee",
   "metadata": {},
   "source": [
    "## <mark>Exercises</mark>\n",
    "\n",
    "### <mark>Exercise 1: Filter and Combine Log Files</mark>\n",
    "\n",
    "You have multiple log files from different servers in the `data/logs/` folder. Your task is to:\n",
    "1. Read all text files from the folder\n",
    "2. Extract only the ERROR and WARNING lines\n",
    "3. Create a DataFrame with columns: `timestamp`, `level`, `message`\n",
    "4. Sort by timestamp\n",
    "\n",
    "Your expected output is as follows:\n",
    "\n",
    "**Expected output:**\n",
    "```\n",
    "   timestamp            level  message                          server\n",
    "0  2024-01-15 10:24:12  ERROR  Database connection failed      server1\n",
    "1  2024-01-15 10:25:03  WARNING High memory usage: 85%         server1\n",
    "2  2024-01-15 10:27:45  ERROR  Timeout on API call             server1\n",
    "3  2024-01-15 10:31:22  WARNING Disk space low: 15%            server2\n",
    "4  2024-01-15 10:32:33  ERROR  Failed to write file            server2\n",
    "5  2024-01-15 10:41:11  ERROR  Network timeout                 server3\n",
    "6  2024-01-15 10:42:22  WARNING CPU usage: 95%                 server3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Your code here\n",
    "# 1. Get all .log files from data/logs/\n",
    "# 2. Read each file and extract ERROR and WARNING lines\n",
    "# 3. Parse each line into timestamp, level, and message\n",
    "# 4. Create a DataFrame and sort by timestamp\n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f676a",
   "metadata": {},
   "source": [
    "### <mark>Exercise 2: Configuration Merger</mark>\n",
    "\n",
    "You have multiple JSON configuration files for different environments (dev, staging, prod). Your task is to:\n",
    "1. Read all JSON files from `data/configs/`\n",
    "2. Merge them into a single DataFrame showing settings across environments\n",
    "3. Identify which settings differ between environments\n",
    "\n",
    "Your expected output is as follows:\n",
    "```\n",
    "                        dev                      staging                  prod\n",
    "database_host           localhost                staging-db.company.com   prod-db.company.com\n",
    "database_port           5432                     5432                     5432\n",
    "database_max_connections 10                      50                       200\n",
    "api_timeout             30                       60                       60\n",
    "api_rate_limit          100                      500                      1000\n",
    "debug                   True                     True                     False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# TODO: Your code here\n",
    "# 1. Read all JSON files from data/configs/\n",
    "# 2. Flatten the nested structure\n",
    "# 3. Create a DataFrame where each row is a setting and columns are environments\n",
    "# 4. Identify settings that differ across environments\n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadd7f6",
   "metadata": {},
   "source": [
    "**Answers**: Uncomment and run the code to see answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/file-1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/file-2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc462e67",
   "metadata": {},
   "source": [
    "## Summary: File Reading Best Practices\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Use `pathlib`** over `os` for modern, cleaner code\n",
    "- **CSV files**: Use `pd.read_csv()` for tabular data\n",
    "- **JSON files**: Use `json.load()` for configs, `pd.read_json()` for tabular data\n",
    "- **Parquet files**: Use `pd.read_parquet()` for efficient storage of large datasets\n",
    "- **Text files**: Use context managers (`with open()`) to ensure files close properly\n",
    "- **YAML files**: Use `yaml.safe_load()` for configuration files\n",
    "- **Multiple files**: Use `pathlib.glob()` + `pd.concat()` to combine datasets\n",
    "\n",
    "**Common patterns:**\n",
    "```python\n",
    "# Read single CSV\n",
    "df = pd.read_csv('file.csv')\n",
    "\n",
    "# Read all CSVs from folder\n",
    "dfs = [pd.read_csv(f) for f in Path('folder').glob('*.csv')]\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Read config file\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "```\n",
    "\n",
    "**Remember**: Always use context managers (`with open()`) when reading files to ensure they're properly closed, even if an error occurs! üìÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
